{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, jsonify, request\n",
    "#from src.helper import download_hugging_face_embeddings\n",
    "\n",
    "#from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.vectorstores import Pinecone as  PineconeVectorStore\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "#from src.prompt import *\n",
    "import os\n",
    "import sentence_transformers\n",
    "\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the pinecone API keys and index here:\n",
    "pinecone_api_key = os.environ.get('PINECONE_API_KEY')\n",
    "pinecone_api_env = os.environ.get('PINECONE_API_ENV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to parse all the documents in the root directory\n",
    "#Each page is read into DirectoryLoader object with each element representing a page\n",
    "def pdf_loader(pdf_dir):\n",
    "    loader = DirectoryLoader(pdf_dir, glob='*.pdf', loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents  \n",
    "#Each page is then split into chunks of max size 500\n",
    "#Also each chunk overlaps each other by 20 tokens\n",
    "def text_splitter(exrtacted_doc):\n",
    "    splitter_obj = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    split_doc = splitter_obj.split_documents(exrtacted_doc)\n",
    "    return split_doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exrtacted_doc = pdf_loader('data')\n",
    "split_doc = text_splitter(exrtacted_doc)\n",
    "print(\"Total vocab size: \", len(split_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeding model to conver the word to embeddings\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "embed_model = download_hugging_face_embeddings()\n",
    "sample_embedding  = embed_model.embed_query(\"This is a testing code for converting words to vectors\")\n",
    "print(\"Total input token vector block size: \", len(sample_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do this step only if you want add a new vector database to pinecon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "med-app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# configure client\\npc = pinecone.Pinecone(api_key=pinecone_api_key)\\nindex_name = \"med-app\"\\nindex = pc.Index(index_name)\\nindex.describe_index_stats()\\n\\n\\n#vectorstore = PineconeVectorStore.from_existing_index(doc_list, embed_model, index_name)\\npc_interface = Pinecone.from_existing_index(\\n    index_name=index_name,\\n    embedding=embed_model,  # Replace with your embedding model\\n    #namespace=\"YOUR_NAMESPACE\"  # Optional\\n)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list = [t.page_content for t in split_doc]\n",
    "\n",
    "#load_dotenv()\n",
    "#import pinecone\n",
    "from langchain.vectorstores import Pinecone as  PineconeVectorStore\n",
    "#Initialize the PineCone database vector\n",
    "pc = pinecone.Pinecone(api_key=pinecone_api_key,\n",
    "              environment=pinecone_api_env)\n",
    "for name in pc.list_indexes().names():\n",
    "    print(name)\n",
    "index_name = \"med-app\"\n",
    "index = pc.Index(index_name)\n",
    "'''\n",
    "# configure client\n",
    "pc = pinecone.Pinecone(api_key=pinecone_api_key)\n",
    "index_name = \"med-app\"\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()\n",
    "\n",
    "\n",
    "#vectorstore = PineconeVectorStore.from_existing_index(doc_list, embed_model, index_name)\n",
    "pc_interface = Pinecone.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embed_model,  # Replace with your embedding model\n",
    "    #namespace=\"YOUR_NAMESPACE\"  # Optional\n",
    ")\n",
    "'''\n",
    "\n",
    "vectorstore = PineconeVectorStore(index, embed_model, \"text\")\n",
    "\n",
    "vectorstore.add_texts(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "The anwers must be very short and consist a maximum of only 200 words\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the prompting template package from LangChain\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\":PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acidosis seeRespiratory acidosis; Renal\n",
      "tubular acidosis; Metabolic acidosis\n",
      "Acne\n",
      "Definition\n",
      "Acne is a common skin disease characterized by\n",
      "pimples on the face, chest, and back. It occurs when thepores of the skin become clogged with oil, dead skincells, and bacteria.\n",
      "Description\n",
      "Acne vulgaris, the medical term for common acne, is\n"
     ]
    }
   ],
   "source": [
    "index_name = \"med-app\"\n",
    "pc = pinecone.Pinecone(api_key=pinecone_api_key,\n",
    "                    environment=pinecone_api_key)\n",
    "#index = pc.Index(index_name)\n",
    "#Loading the index\n",
    "docsearch = PineconeVectorStore.from_existing_index(index_name, embed_model)\n",
    "\n",
    "query = \"What is acne?\"\n",
    "result = docsearch.similarity_search(query, k=3)\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Langchain chain concept \n",
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\medapp\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response : \n",
      "Cancer is a disease caused by abnormal cell growth that can invade and damage nearby tissues and organs. The two most common types of cancer found in men are basal cell carcinomas and malignant melanomas, both of which are skin cancers.\n"
     ]
    }
   ],
   "source": [
    "user_input=\"what is cancer? What are the most common cancer types found in men?\"\n",
    "result=qa({\"query\": user_input})\n",
    "print(\"Response : \")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    result=qa({\"query\": user_input})\n",
    "    print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Tharun\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "token=\"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\"\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tetsing purpose on Google gemma 2b\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
